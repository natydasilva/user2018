---
title: <span style="color:#000000">Interactive Graphics for Visually Diagnosing Forest Classifiers in R  </span>
author: <span style="color:#000000"> <font size="6">Natalia da Silva, UDELAR-IESTA</span></font>

date: <span style="color:#000000"> 13 July, 2018
output: ioslides_presentation
bibliography: bibliophd.bib
---



 <style>
 .title-slide {
     background-image: url(figure/paint1.png);
     background-repeat: no-repeat;
     padding:40px;
     background-size: 1000px 800px;
   }
   </style>
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```


## <span style="color:#26734d">Estructura de la presentación</span> 
- Motivation

- Ensemble models

- PPforest, Projection pursuit random forest

- Visually exploring a PPforest object

- Final comments


## <span style="color:#26734d">Motivation</span> 

- Supervise learning: objective is to predict a categorical variable then it is a classification problem (two class or multiclass problem)

- **PPforest** is a new supervised method based on bagged projection pursuit trees for classification problems.

- This method improves the predictive performance when the separation between classes is in combinations of variables.

## <span style="color:#26734d"> Ensemble models</span> 
- Ensembles learning methods: combined multiple individual models trained independently to build a prediction model.

- Some well known examples of ensemble learning methods are, boosting [@schapire1990strength], bagging [@breiman1996bagging] and random forest [@breiman2001random] among others.

- Main differences between ensembles, type of individual models to be combined and  the ways these individual models are combined. 

## <span style="color:#26734d">PPforest</span>  
**PPforest** is an ensemble learning method, built on bagged trees.  

Main concepts:

* Bootstrap aggregation (@breiman1996bagging and @breiman1996heuristics)

* Random feature selection (@amit1997shape and @ho1998random) to individual classification trees for prediction.


## <span style="color:#26734d">PPforest, individual classifiers</span>  
The individual classifier in **PPforest** is a **PPtree** [@lee2005projection].

The splits in **PPforest** are based on a linear combination of randomly chosen variables.
Utilizing linear combinations of variables the individual model (**PPtree**) separates classes taking into account the correlation between variables.

## <span style="color:#26734d"> PPtree, individual classifier for PPforest  </span> 
PPtree Combines tree structure methods with projection pursuit dimension reduction. 
PPtree treats the data always as a two-class system.

When the classes are more than two the algorithm uses a two step  projection pursuits optimization in every node split.



## <span style="color:#26734d"> PPtree: Illustration of PPtree algorithm</span>
<img src="figure/diag2.png" width="700px"/>


## <span style="color:#26734d"> CART vs PPtree,  simulated data</span>
```{r libs, echo = FALSE, warning = FALSE, message=FALSE,fig.align="center"}
library(MASS)
library(ggplot2)
library(RColorBrewer)
library(PPtreeViz)
library(gridExtra)
library(reshape2)
library(PPforest)
library(plyr)
library(plotly)
library(dplyr)
library(GGally)
library(tidyr)

```
<center>
```{r, echo = FALSE, fig.height = 4, fig.width = 7, cache = TRUE, warning = FALSE, message=FALSE,fig.align="center", strip.white=TRUE }
simu3 <- function(mux1, mux2, muy1, muy2, muz1, muz2,  cor1,cor2,cor3) {
  set.seed(666)
  bivn <- mvrnorm(100, mu = c(mux1, mux2), Sigma = matrix(c(1, cor1, cor1, 1), 2))
  bivn2 <- mvrnorm(100, mu = c(muy1, muy2), Sigma = matrix(c(1, cor2, cor2, 1), 2))
  bivn3 <- mvrnorm(100, mu = c(muz1, muz2), Sigma = matrix(c(1, cor3, cor3, 1), 2))

  d1 <- data.frame(Sim = "sim1", bivn)
  d2 <- data.frame(Sim = "sim2", bivn2)
  d3 <- data.frame(Sim = "sim3", bivn3)
  rbind(d1, d2, d3)
}

dat.pl2 <- simu3(-1,0.6,0,-0.6, 2,-1,0.95, 0.95, 0.95)

grilla <- base::expand.grid(X1 = seq(-4,4.8,,100), X2 = seq(-4.3,3.3,,100))

pptree <- PPtreeViz::PPTreeclass(Sim~., data = dat.pl2, "LDA")
ppred.sim <- PPtreeViz::PPclassify(pptree, test.data = grilla, Rule = 1)
grilla$ppred<-ppred.sim[[2]]

rpart.crab <- rpart::rpart(Sim ~ X1 + X2, data = dat.pl2)
rpart.pred <- predict(rpart.crab, newdata = grilla, type="class")

p <- ggplot2::ggplot(data = grilla ) +
  ggplot2::geom_point(ggplot2::aes(x = X1, y = X2, color = as.factor(ppred),shape=as.factor(ppred)),alpha = .20)+
  geom_abline(intercept= pptree$splitCutoff.node[[1]]/pptree$projbest.node[[3]], slope= -pptree$projbest.node[[1]]/pptree$projbest.node[[3]], size=1 )+ scale_colour_brewer(name="Class",type="qual",palette="Dark2")+ggplot2::theme_bw() +
  geom_abline(intercept= pptree$splitCutoff.node[[2]]/pptree$projbest.node[[4]], slope=-pptree$projbest.node[[2]]/pptree$projbest.node[[4]],size=1)+ scale_shape_discrete(name='Class')

pl.pp <- p + ggplot2::geom_point(data = dat.pl2, ggplot2::aes(x = X1 , y = X2, group= Sim, shape = Sim, color=Sim), size = I(3)  ) + theme(legend.position = "bottom", legend.text = element_text(size = 6), aspect.ratio = 1) + scale_y_continuous(expand = c(0,0)) + scale_x_continuous(expand = c(0,0)) + 
  labs(title = "PPtree (PPtreeViz)")

p2 <- ggplot2::ggplot(data = grilla ) + ggplot2::geom_point(ggplot2::aes(x = X1, y = X2  , color = as.factor(rpart.pred),shape =  as.factor(rpart.pred)), alpha = .2) +
  ggplot2::scale_colour_brewer(name = "Class",labels = levels(dat.pl2$Sim),type="qual",palette="Dark2") +
  ggplot2::theme_bw() + scale_shape_discrete(name='Class')+ scale_y_continuous(expand = c(0,0)) + scale_x_continuous(expand = c(0,0))

pl.rpart <- p2 + ggplot2::geom_point(data = dat.pl2, ggplot2::aes(x = X1 , y = X2, group=Sim,shape = Sim, color=Sim), size = I(3)  ) + theme(legend.position = "bottom", legend.text = element_text(size = 6), aspect.ratio = 1) +
  labs(title = "CART (rpart)")

grid.arrange(pl.rpart,pl.pp,ncol=2)

```
</center>



## <span style="color:#26734d"> PPforest Illustration </span> 
<img src="figure/diagram.png" width="700px"/>


## <span style="color:#26734d"> Implementation </span> 

* Initial version was develop entirely in R, not fast enought
* Two code optimization strategies were employed:
     * translate the main functions to Rcpp
     * parallelization 

## <span style="color:#26734d"> PPforest runing time </span> 
<!-- <img src="figure/resus-1.pdf" vspace="1"  "width="700px"/> -->

```{r echo=FALSE, warning=FALSE, message=FALSE}
load("preformance_timesWTplyr.Rdata")
global_labeller <- labeller(
 g = class,
 ntrees = trees,
 .default = label_both
)
mm.allbig <- bind_rows(mm.new, mm.old) %>%
  rename(p=ps, g=gs, n=ns, ntrees=mtrees) %>%
  mutate(version = recode(version, new="post", old="pre")) %>%
  mutate(seconds = time/1e9)
mm.allbig$version <- factor(mm.allbig$version, levels = c("pre","post"))
levels(mm.allbig$version) <- c("Only in R", "Rcpp & parallelization")
ggplot(data = mm.allbig) +
    geom_smooth(aes(g, seconds, color = version,
                        linetype = version), method="lm") +
    geom_jitter(aes(g, seconds, color = version, shape = version),
               alpha = .4, size = 3, height = 0) +
    scale_x_continuous(breaks=seq(0,10,2)) +
    labs(x = "Num. groups", y = "Time (sec)") +
    scale_colour_brewer(name = "", type = "qual",
                      palette = "Dark2") +
    scale_shape(name = "") +
    scale_linetype(name = "") +
    facet_grid(ntrees~n + p, labeller = label_both,
                scales = "free_y") +
    #scale_x_continuous(trans = 'log2') +
    theme(legend.position = "bottom",
        axis.text = element_text(size = 6), aspect.ratio = 1 )

```

## <span style="color:#26734d"> PPforest Diagnostics </span> 

* OOB Error rate
<!-- : OOB error, proportion of times that case $n$ is wrong classified  averaged over all cases is the oob error estimate.  -->

* Variable importance

* Proximity matrix

<!-- :calculated for every pair of observations, if cases $k_i$ and $k_j$ are in the same terminal node increase their proximity by one. Normalize  by the number of trees.  -->

* Vote matrix
<!-- : one row for each input data point and one column for each class, giving the number of (OOB) votes for each class.   -->

<!-- Segunda motivación, construir herramientas para explorar, entender y diagnosticar "black box models" -->

## <span style="color:#26734d">Visually diagnosing forest classifiers </span> 
Structuring data and constructing plots to explore forest classification models interactively.

 We proposed a method to explore and diagnostic ensemble classifiers based on three levels of analysis:
 
 1. **Individual cases (Observations)** 
 2. **Individual models (Trees)** 
 3. **Performance comparison (PPF vs RF)**
 
Key part of the visualization is the use of interactive visualization methods. 

Interactive web-based visualization of the ensemble methods.


## <span style="color:#26734d">Interactive graphics</span>
 Two key components that an interactive graphic should accomplish: 
 
* Interactions in each visualization
* Links between different graphics

Why we should use interactive visualizations?

To see connections inside each level that cannot be seen in a static graphs.
 <!--  -->
 
## <span style="color:#26734d">Interactive graphics</span>


- Links at case level allows to identify cases where the model is not working properly and allows to characterize this case base on the original data. 

- Also we can identify individual models in the ensemble that are not good enough and why this is happening. 

- The last level of analysis focused on model comparison based on predicted performance by class. 


## <span style="color:#26734d"> Fishcatch data example </span>

159 fishes of 7 species (Bream, Parkki, Perch, Pike, Roach, Smelt and Whitewish) are caught and measured, 6 variables. 

Varible|Description
------------|------------------------------------------------------
weight| Weight of the fish (in grams)
length1| Length from the nose to the beginning of the tail (in cm)
length2| Length from the nose to the notch of the tail (in cm)
length3| Length from the nose to the end of the tail (in cm)
height| Maximal height as % of Length3
width| Maximal width as % of Length3


## <span style="color:#26734d">Panel 1: Individual cases</span>
<img src="figure/fishT1.png" height="450px" width="700px"/>

## <span style="color:#26734d">Example: Fishcatch </span>

```{r hola, echo = FALSE, fig.height = 5, fig.width = 8, warning = FALSE, message=FALSE,fig.align="center",  warning = FALSE, message=FALSE}
library(devtools)
#install_github("natydasilva/PPforest")
library(PPforest)
library(ggplot2)
library(tidyr)
library(dplyr)
library(plotly)
library("vembedr")
load("ppf3.RData")


ppf <- ppf3

myscale <- function(x) (x - mean(x)) / sd(x)
scale.dat <- ppf$train %>% mutate_each(funs(myscale),-matches(ppf$class.var)) 
scale.dat.melt <- scale.dat %>%  mutate(ids = 1:nrow(ppf$train)) %>% gather(var,Value,-Type,-ids)
scale.dat.melt$Variables <- as.numeric(as.factor(scale.dat.melt$var))
colnames(scale.dat.melt)[1] <- "Class"
scale.dat.melt$Value <- round(scale.dat.melt$Value ,3)

p <- ggplot(scale.dat.melt, aes(x = Variables, y = Value, 
                                    group = ids, key = ids, colour = Class, var = var)) +
      geom_line(alpha = 0.3) + scale_x_discrete(limits = levels(as.factor(scale.dat.melt$var)), expand = c(0.01,0.01)) +
      ggtitle("Data parallel plot ") + theme(legend.position = "none", axis.text.x  = element_text(angle = 90, vjust = 0.5)) + 
      scale_colour_brewer(type = "qual", palette = "Dark2")
 plotly::ggplotly(p,tooltip = c("var","colour","y","key"))   

```

## <span style="color:#26734d">Proximity Matrix</span>

<center>
```{r, echo=FALSE,fig.height=5,fig.width=5,  warning = FALSE, message=FALSE,fig.align="center", cache=TRUE}
k <- 2
id <- diag(dim(ppf$train)[1])
id[lower.tri(id, diag = TRUE)] <- 1 - ppf[[9]]$proxi
rf.mds <- stats::cmdscale(as.dist(id), eig = TRUE, k = k)
colnames(rf.mds$points) <- paste("MDS", 1:k, sep = "")
nlevs <- nlevels(ppf$train[, 1])



f.helmert <- function(d)
{
  helmert <- rep(1 / sqrt(d), d)
  for (i in 1:(d - 1))
  {
    x <- rep(1 / sqrt(i * (i + 1)), i)
    x <- c(x,-i / sqrt(i * (i + 1)))
    x <- c(x, rep(0, d - i - 1))
    helmert <- rbind(helmert, x)
  }
  return(helmert)
}

#projected data
projct <- t(f.helmert(length(unique(ppf$train[,ppf$class.var])))[-1,])

bestnode <- plyr::ldply(ppf[[8]][[2]], function(x) {
  bn <- abs(x$projbest.node)
  bn[bn == 0] <- NA
  dat.fr <- data.frame(node = 1:dim(x$projbest.node)[1],bn)
  
})

colnames(bestnode)[-1] <- colnames(ppf$train[,-which(colnames(ppf$train)==ppf$class.var)])
bestnode$node <- as.factor(bestnode$node)


data <- data.frame(
        MDS1 = rf.mds$points[,1], MDS2 = rf.mds$points[,2],
        Class = ppf$train[, 1],ids = 1:nrow(rf.mds$points),
        fill = logical(nrow(ppf$train)),proj.vote =
          as.matrix(ppf$votes) %*% projct,
        vote = ppf$votes, pred = ppf$prediction.oob, scale.dat
        )
data$MDS1 <- round(data$MDS1,3)
data$MDS2 <- round(data$MDS2,3)


p2 <- ggplot(data = data, aes(x = MDS1, y = MDS2, 
                                    colour = Class, key = ids)) + 
      geom_point(size = I(3),alpha = .5)  + theme(legend.position = "none", legend.text = element_text(angle = 90), legend.key = element_blank(), aspect.ratio =
        1)  + labs(y = "MDS 2", x = "MDS 1", title = "Multidimensional Scaling") +
     scale_colour_brewer(type = "qual",palette = "Dark2")
    
ggplotly(p2,tooltip = c("colour","x","y","key"))

```
</center>


## <span style="color:#26734d"> Vote matrix, Jittered side-by-side dot plot</span>
<center>
```{r, echo=FALSE,fig.height=5,fig.width=7,fig.align="center", warning=FALSE,message=FALSE}
library(stringr)
 reddat <- data %>% 
            select(ids, Class, starts_with("vote"), pred) 
      colnames(reddat) <- colnames( reddat) %>% str_replace("vote.","")
   
      sidepl <- reddat %>% gather(classpred, Proportion, -pred, -ids, -Class) %>% mutate(Proportion= round(Proportion,3))
   
  
    p <- ggplot(data = sidepl, aes(classpred, Proportion, colour = Class, key = ids)) + 
      geom_jitter(height = 0, size = I(3), alpha = .5) +
      theme(axis.text.x  = element_text(angle = 45, vjust = 0.5), aspect.ratio = 1) +
      labs(x = "Class", title = "Side by side plot", y ="Proportion", color="True class") + scale_colour_brewer(type = "qual", palette = "Dark2") 
     ggplotly(p,tooltip = c("colour","y","key")) %>% layout(dragmode = "select")
```
</center>


<!-- ## <span style="color:#26734d"> Vote matrix, ternary plot</span> -->

<!-- * Diagrama triangular muestra la proporción de tres variables que suman una constante y lo hace usando coordinadas baricéntricas. -->
<!-- Podemos dibujar la proporción de tres variables en dos dimensiones. Es útil para visualizar datos composicionales. -->

<!-- * Con más de tres clases el diagrama triangular necesita ser generalizado. -->
<!-- Los datos son proyectados en un espacio $(p-1)-$D. Este método será utilizado para visualizar la matriz de votos. -->

## <span style="color:#26734d"> Vote matrix, ternary plot</span>
<center>
```{r, echo=FALSE,fig.height=3,fig.width=7,fig.align="center",cache=TRUE, warning = FALSE, message=FALSE}


f.helmert <- function(d)
{
  helmert <- rep(1 / sqrt(d), d)
  for (i in 1:(d - 1))
  {
    x <- rep(1 / sqrt(i * (i + 1)), i)
    x <- c(x, -i / sqrt(i * (i + 1)))
    x <- c(x, rep(0, d - i - 1))
    helmert <- rbind(helmert, x)
  }

  return(helmert)
}

makePairs <- function(dat, id = c(a, b, c)) {
  aux <- dat[,-c(1, 2)]

  d <- aux[, id]

  grid <- expand.grid(x = id, y = id)
  grid <- subset(grid, x != y)
  all <- do.call("rbind", lapply(1:nrow(grid), function(i) {
    xcol <- grid[i, "x"]
    ycol <- grid[i, "y"]
    data.frame(
      Class = dat[, 1],
      ids = dat[, 2],
      x = dat[, xcol+2],
      y = dat[, ycol+2],
      pair = paste(grid[i, ], collapse = '-')
    )
  }))

  all
}

#ppf PPforest object
#V1,v2,v3 select the 3 proj directions
ternarydata <- function(ppf, v1, v2, v3){
  n.class <- ppf$train %>% select_(ppf$class.var) %>% unique() %>% nrow()
  projct <- t(f.helmert(length(unique(ppf$train[, ppf$class.var])))[-1,])

  dat3 <-
    data.frame(
      Class = ppf$train[, ppf$class.var],
      ids = 1:nrow(ppf$train),
      proj.vote = round(as.matrix(ppf$votes) %*% projct,3)
    )

  ##with 3 or less classes
  empt <- rep(1:nrow(dat3), 3)
  #dat3.empt <- dat3[empt, ] %>% mutate(rep = rep(1:3, each = nrow(dat3)))
  if (n.class > 3) {
    gg1 <-  makePairs(dat3, c(v1,  v2, v3))
  }

  gg1 <-  makePairs(dat3, id = c(v1, v2, v3))

  return(gg1)
}



#need to run f_hermite
f_composition <- function(data) {
  d <- dim(data)[2]
  hm <- f.helmert(d)
  x <- data - matrix(1 / d, dim(data)[1], d)
  return((x %*% t(hm))[,-1])
}

simplex <- function(p = 3) {
  vert <- f_composition(diag(p + 1))
  colnames(vert) <- paste0("d", 1:ncol(vert))

  wires <-
    do.call(expand.grid, list(c(1:nrow(vert)), c(1:nrow(vert))))

  structure(list(points = vert,
                 edges = wires[!(wires[, 1] == wires[, 2]),]))
}

##ternary plot
ternaryshell <- function(ppf, sp = 3, dx = 1, dy = 2, v1 = 1, v2 = 2, v3 = 3){
  s <- simplex(sp)
  pts <- data.frame(s$points)
  #pts <- data.frame(s$points[,c(dx,dy)])
  gg1 <- ternarydata(ppf, v1, v2, v3)

  edg <- data.frame(x1=pts[,dx][s$edges[,1]], x2 = pts[,dx][s$edg[,2]],
                    y1=pts[,dy][s$edg[,1]], y2 = pts[,dy][s$edg[,2]])

  p1  <- gg1 %>% filter(pair %in% paste(dx,dy, sep="-") ) %>%
    select(Class, x, y) %>%
    ggplot(aes(x, y, color = Class)) +
      geom_segment(data = edg, aes(x = x1, xend = x2,
                                 y = y1, yend = y2), color = "black" ) +
      geom_point(size = I(3), alpha = .5) +
      labs(y = "",  x = "") +
      theme(legend.position = "none", aspect.ratio = 1) +
      scale_colour_brewer(type = "qual", palette = "Dark2") +
      labs(x = paste0("T",dx,""), y = paste0("T",dy,"")) +
    theme(aspect.ratio=1)

  p1
}

t1 <- ternaryshell(ppf, 6, 1, 2)
t2 <- ternaryshell(ppf, 6, 1, 3)
t3 <- ternaryshell(ppf, 6, 2, 3)
 subplot(t1,t2,t3, titleX=TRUE, titleY=TRUE)
```
</center>





## <span style="color:#26734d">Panel 1: App for individual cases  </span>

<iframe class="vimeo-embed" src="https://player.vimeo.com/video/222028803" width="500" height="281" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen> </iframe>


## <span style="color:#26734d">Panel 2: Individual models</span>

<img src="figure/fishT2.png" height="450px" width="700px"/>

## <span style="color:#26734d">Panel 2: App for individual models </span>

<iframe class="vimeo-embed" src="https://player.vimeo.com/video/222029591" width="500" height="281" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen> </iframe>


## <span style="color:#26734d">Panel 3: Performance comparison (PPF vs RF) </span>

<img src="figure/fishT3.png" height="450px" width="700px" />

## <span style="color:#26734d">Panel 3: App for performance comparison (PPF vs RF) </span>

<iframe class="vimeo-embed" src="https://player.vimeo.com/video/222029620" width="500" height="281" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen> </iframe>


<!-- ##<span style="color:#26734d">PPforest performance and comparison</span>  -->

<!-- * Simulation study to understand the PPF performance relative to RF -->
<!-- * Two parameters are varied, $\sigma$ and $\theta$  -->

<!-- <img src="figure/simulation.pdf" width="300px"/> -->

<!-- ##<span style="color:#26734d">PPforest performance and comparison</span>  -->
<!-- <img src="figure/simulation-1.pdf" width="700px"/> -->


<!-- ##<span style="color:#26734d">PPforest performance and comparison</span>  -->
<!-- - Randomly chosen 2/3 observations in training and  1/3 in test to compute predictive error -->
<!-- - Repeat 200 times -->
<!-- ```{r parbench, depenson="benchpl",echo=FALSE, message=FALSE, warning=FALSE, fig.height=6, fig.width=11} -->
<!-- load("table.raw.Rdata") -->
<!-- aux <- table.raw %>% -->
<!--   separate(method, into = c('met', 'b') ) %>% -->
<!--   group_by(data, met) %>% mutate(mm = min(mn.te) ) %>% -->
<!--   filter(mn.te == mm ) %>% select(data, met, mn.tr, mn.te) %>% ungroup() %>% -->
<!--   gather(type, error, mn.tr, mn.te) %>% mutate(met = reorder(met, error) ) %>% -->
<!--   filter(type=="mn.te") %>%unite(gg, data, type, remove = FALSE) %>% -->
<!--   mutate(type = factor(type, labels = "Test"), error = round(error,3)) %>% -->
<!--   #mutate(type = factor(type, labels = c("Training", "Test")), error = round(error,3)) %>%  -->
<!--   filter(type == "Test")  -->
<!--   p1 <- ggplot(aux) + geom_line(aes(x = met, y = error, group = gg, color = data) ) + -->
<!--   scale_x_discrete( expand = c(0.01, 0.01) ) + -->
<!--  # scale_colour_brewer(palette="Dark2") + -->
<!--   labs(y ="Average error rate", x ="Method", colour ="Data")  -->
<!--   #facet_wrap(~type, ncol=2, labeller = label_parsed)  -->
<!--  # theme(legend.position = "bottom") -->
<!-- ggplotly(p1, tooltip = c("met", "error","data"))  -->
<!-- ``` -->
<!-- \caption{Benchmark data results shown graphically. PPF performs consistently well across most of the data sets. \label{parallel}} -->
<!-- \end{figure} -->


<!-- ##<span style="color:#26734d">PPforest performance and comparison</span>  -->

<!-- ```{r echo=FALSE, message=FALSE, cache=TRUE, eval=FALSE} -->
<!-- library(ggplot2) -->
<!-- library(purrr) -->
<!-- library(dplyr) -->
<!-- library(tidyr) -->
<!-- library(knitr) -->
<!-- library(kableExtra) -->

<!-- # load data -->
<!-- ll <- paste('benchmark', list.files('benchmark', pattern = '.ata'), sep='/') -->
<!-- for( i in 1:length(ll) ) load(ll[i]) -->

<!-- # put all data as part of a list -->
<!-- dtnm <- data_frame(nm = list.files('benchmark', pattern = '.ata')) %>% -->
<!--   separate(nm, into=c('nm2', 'mugre'), sep='\\.') -->

<!-- dts <- list() -->
<!-- for (i in 1:length(ll) ) dts[[i]] <- get(dtnm$nm2[i]) -->
<!-- names(dts) <- dtnm$nm2 -->

<!-- ff <- function(x) {  -->
<!--   mm <- c('cart', 'ppf', 'pptr', 'rf') -->
<!--   xi <- list() -->
<!--   for ( i in 1:4) xi[[i]]<- x %>% slice( grep(mm[i], x$method) ) %>% slice(which.min(mn.te)) -->

<!--   bind_rows(xi) %>% mutate(met = mm) %>% -->
<!--     select(met, mn.tr, mn.te) %>%  -->
<!--     gather( type, err, mn.tr, mn.te) %>%  -->
<!--     unite(mm, type, met, sep='_') %>% -->
<!--     spread(mm, err) -->
<!--   } -->

<!-- dt2 <- bind_rows(lapply(dts, ff), .id='data') %>%  -->
<!--   separate(data, into=c('Data','mugre' )) %>% select(-mugre) -->
<!-- dt2 <- dt2[, c(1, 6:9, 2:5)]  -->
<!-- colnames(dt2) <- c('Data', 'CART', 'PPforest', 'PPtree', 'RForest', 'CART', 'PPforest', 'PPtree', 'RForest' ) -->

<!-- # using xtable for latex output -->
<!-- # library(xtable) -->
<!-- #mcol =  list(pos=list(-1), command="& \\multicolumn{4}{c}{TRAINING} & \\multicolumn{4}{c}{TEST} \\\\") -->
<!-- #xtable(dt, digits=3, align=c('l', 'l||', 'r', 'r', 'r', 'r||' , 'r', 'r', 'r', 'r') ) %>% print(add.to.row=mcol, include.rownames=FALSE) -->

<!-- # for r-markdown html output -->
<!-- kable(dt2, digits = 2, format='html') %>% -->
<!--   kable_styling(c("striped", "bordered")) %>% -->
<!--   add_header_above(c(" " , "TRAINING" = 4, "TEST" = 4)) -->
<!-- ``` -->



<!-- ## <span style="color:#26734d">Final comments </span> -->
<!-- 1. PPF método de clasificación agregado basado en árboles con particiones oblicuas. -->

<!-- 2. PPF toma la correlación entre variables en cuenta y mejora la performance de RF cuando la separación ocurre en combinaciones de variables. -->

<!-- 3. En estudios de simulación PPF tiene mejor performance que RF cuando las clases pueden ser separadas por combinaciones lineales y la correlación entre variables se incrementa. -->

## <span style="color:#26734d">Final comments</span>

1. Having better tools to open up black box models will provide for better understanding the data, the model strengths and weaknesses, and how the model will performs on future data.

2. This visualisation app provides a selection of interactive plots to diagnose PPF models. 

3. This shell could be used to make an app for other ensemble classifiers. 

4. Combining `shiny`, `ggplot2` and `plotly` we can develop informative interactive visualizations

<!-- 4. The app is implemented with new technology for interactive graphics provided by the `plotly` package. It is one of the first uses of these new tools. -->


<!-- ## <span style="color:#26734d">Some PPforest extensions </span> -->

<!-- 1. Some enhancements of the PPtree algorithm to get a better separation of classes more broadly -->
<!-- 2. Allowing a tree with multiple splits per class can help to tackle nonlinear classification problems. -->

<!-- 3. Viz app extensions that could help it to be a tool for model refinement: -->
<!--     <!-- * Using the diagnostics to weed out under-performing models in the ensemble --> 
<!--     <!-- * Identifying and boosting models that perform well, particularly if they do well for problematic subsets of the data --> 
<!--     * Problematic cases could be removed, and ensembles re-fit -->
<!--     * This tool can be extended to other ensembles, and perhaps a new R package that provides general tools to explore other ensembles will be an useful tool -->
<!--     <!-- * Classes as a whole could be aggregated or re-organised as suggested by the model diagnostics -->


## <span style="color:#26734d"> Information </span>

1. PPforest package:https://cran.r-project.org/web/packages/PPforest/index.html
2.  Slides:https://github.com/natydasilva/user2018
3.  Viz: https://natydasilva.shinyapps.io/shinyppforest/
4.  webpage: http://natydasilva.com
5.  email: natalia\@iesta.edu.uy  twitter:\@pacocuak


## Bibliografía
